\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
%\VignetteIndexEntry{glmbayes Package: An Introduction}
%\VignetteEngine{Sweave}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

First we install the glmbayes library.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(glmbayes)}
\end{alltt}
\end{kframe}
\end{knitrout}

To understand how the output of the glmb function mirrors that for the glm function, it is useful to take a look at the first portion of the example that is provided with the glm function. The data is based on Randomized Controlled Trial data from Dobson (1990). Here is a view of the data.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Dobson (1990) Page 93: Randomized Controlled Trial :}
\hlstd{counts} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{18}\hlstd{,}\hlnum{17}\hlstd{,}\hlnum{15}\hlstd{,}\hlnum{20}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{20}\hlstd{,}\hlnum{25}\hlstd{,}\hlnum{13}\hlstd{,}\hlnum{12}\hlstd{)}
\hlstd{outcome} \hlkwb{<-} \hlkwd{gl}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{9}\hlstd{)}
\hlstd{treatment} \hlkwb{<-} \hlkwd{gl}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{)}
\hlkwd{print}\hlstd{(d.AD} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(treatment, outcome, counts))}
\end{alltt}
\begin{verbatim}
##   treatment outcome counts
## 1         1       1     18
## 2         1       2     17
## 3         1       3     15
## 4         2       1     20
## 5         2       2     10
## 6         2       3     20
## 7         3       1     25
## 8         3       2     13
## 9         3       3     12
\end{verbatim}
\end{kframe}
\end{knitrout}

The example code for the glm function specifies a Poisson regression model based on the formula: counts ~ outcome + treatment.  The resulting model has an intercept, 


To run a Bayesian version of this model, we first need to add a prior. As the output above had 5 columns, we need a prior mean with 5 components. For now, we use log(mean(counts)) as a prior point estimate for the intercept and 0 as point estimates for the other components. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mu}\hlkwb{<-}\hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{5}\hlstd{)}
\hlstd{mu[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]}\hlkwb{=}\hlkwd{log}\hlstd{(}\hlkwd{mean}\hlstd{(counts))}
\hlstd{mu}
\end{alltt}
\begin{verbatim}
##          [,1]
## [1,] 2.813411
## [2,] 0.000000
## [3,] 0.000000
## [4,] 0.000000
## [5,] 0.000000
\end{verbatim}
\end{kframe}
\end{knitrout}

For now, we give all of the components a prior standard deviation of 1 and use it to populate a diagonal prior Variance-Covariance matrix.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mysd}\hlkwb{<-}\hlnum{1}
\hlstd{V}\hlkwb{=}\hlstd{((mysd)}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{*}\hlkwd{diag}\hlstd{(}\hlnum{5}\hlstd{)}
\hlstd{V}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    0    0    0    0
## [2,]    0    1    0    0    0
## [3,]    0    0    1    0    0
## [4,]    0    0    0    1    0
## [5,]    0    0    0    0    1
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{glm.D93} \hlkwb{<-} \hlkwd{glm}\hlstd{(counts} \hlopt{~} \hlstd{outcome} \hlopt{+} \hlstd{treatment,}
              \hlkwc{family} \hlstd{=} \hlkwd{poisson}\hlstd{())}
\end{alltt}
\end{kframe}
\end{knitrout}

The printed output from the call to glm looks as follows (note there are 5 variables in the model). One of the coefficients represents the intercept, while the others represent the effect of outcome and treatment on the counts.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(glm.D93)}
\end{alltt}
\begin{verbatim}
## 
## Call:  glm(formula = counts ~ outcome + treatment, family = poisson())
## 
## Coefficients:
## (Intercept)     outcome2     outcome3   treatment2   treatment3  
##   3.045e+00   -4.543e-01   -2.930e-01    1.338e-15    1.421e-15  
## 
## Degrees of Freedom: 8 Total (i.e. Null);  4 Residual
## Null Deviance:	    10.58 
## Residual Deviance: 5.129 	AIC: 56.76
\end{verbatim}
\end{kframe}
\end{knitrout}


We are now ready to call the glmb function using similar code to that for glm. In addition to the two prior components, we also tell the function to generate 1000 random samples for the analysis (similar to how functions like rnorm would be called). 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n}\hlkwb{<-}\hlnum{1000}
\hlstd{glmb.D93}\hlkwb{<-}\hlkwd{glmb}\hlstd{(}\hlkwc{n}\hlstd{=n,counts} \hlopt{~} \hlstd{outcome} \hlopt{+} \hlstd{treatment,}
               \hlkwc{family} \hlstd{=} \hlkwd{poisson}\hlstd{(),}\hlkwc{mu}\hlstd{=mu,}\hlkwc{Sigma}\hlstd{=V)}
\end{alltt}
\begin{verbatim}
## Standardizing the model:
## Starting Envelope Creation:
## Gridtype is :1
## Number of Variables in model are :5
## Number of points in Grid are :243
## Finding Values of Log-posteriors:
## Finding Value of Gradients at Log-posteriors:
## Finished Log-posterior evaluations:
## Finished Envelope Creation:
\end{verbatim}
\end{kframe}
\end{knitrout}

We can now take a look at the printed output for the glmb function and note that it mirror's what we saw for the glm function above [Change digits default here].

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(glmb.D93)}
\end{alltt}
\begin{verbatim}
## 
## Call:  glmb(n = n, formula = counts ~ outcome + treatment, family = poisson(), 
##     mu = mu, Sigma = V)
## 
## Posterior Mean Coefficients:
## (Intercept)     outcome2     outcome3   treatment2   treatment3  
##    3.007997    -0.439683    -0.275736     0.019163    -0.004407  
## 
## Effective Number of Parameters: 4.558103 
## Expected Residual Deviance: 9.786093 
## DIC: 55.97637
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(glm.D93)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## glm(formula = counts ~ outcome + treatment, family = poisson())
## 
## Deviance Residuals: 
##        1         2         3         4         5         6         7         8  
## -0.67125   0.96272  -0.16965  -0.21999  -0.95552   1.04939   0.84715  -0.09167  
##        9  
## -0.96656  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  3.045e+00  1.709e-01  17.815   <2e-16 ***
## outcome2    -4.543e-01  2.022e-01  -2.247   0.0246 *  
## outcome3    -2.930e-01  1.927e-01  -1.520   0.1285    
## treatment2   1.338e-15  2.000e-01   0.000   1.0000    
## treatment3   1.421e-15  2.000e-01   0.000   1.0000    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 10.5814  on 8  degrees of freedom
## Residual deviance:  5.1291  on 4  degrees of freedom
## AIC: 56.761
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(glmb.D93)}
\end{alltt}
\begin{verbatim}
## Call
## glmb(n = n, formula = counts ~ outcome + treatment, family = poisson(), 
##     mu = mu, Sigma = V)
## 
## Expected Deviance Residuals:
##        1        2        3        4        5        6        7        8 
## -0.52820  1.02394 -0.11549 -0.16185 -0.96581  1.02774  1.01889 -0.01801 
##        9 
## -0.89896 
## 
## Prior and Maximum Likelihood Estimates with Standard Deviations
## 
##             Prior Mean   Prior.sd  Max Like. Like.sd
## (Intercept)  2.813e+00  1.000e+00  3.045e+00   0.171
## outcome2     0.000e+00  1.000e+00 -4.543e-01   0.202
## outcome3     0.000e+00  1.000e+00 -2.930e-01   0.193
## treatment2   0.000e+00  1.000e+00  1.338e-15   0.200
## treatment3   0.000e+00  1.000e+00  1.421e-15   0.200
## 
## Bayesian Estimates Based on 1000 iid draws
## 
##             Post.Mode Post.Mean   Post.Sd MC Error Pr(tail)  
## (Intercept)  3.027191  3.007997  0.159878        0   0.1059  
## outcome2    -0.428953 -0.439683  0.196844        0   0.0130 *
## outcome3    -0.272569 -0.275736  0.196388        0   0.0819 .
## treatment2   0.004042  0.019163  0.184747        0   0.4585  
## treatment3   0.004042 -0.004407  0.185666        0   0.4985  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Distribution Percentiles
## 
##                   1.0%       2.5%       5.0%     Median      95.0%      97.5%
## (Intercept)  2.6230813  2.6915309  2.7561487  3.0056965  3.2668026  3.3052487
## outcome2    -0.8933500 -0.8211660 -0.7661215 -0.4392009 -0.1138020 -0.0408785
## outcome3    -0.7107920 -0.6501652 -0.5956323 -0.2770641  0.0415381  0.0845974
## treatment2  -0.4114017 -0.3230641 -0.2940852  0.0246325  0.3028095  0.3546182
## treatment3  -0.4031580 -0.3522902 -0.2994844 -0.0006627  0.3046038  0.3584036
##             99.0%
## (Intercept) 3.373
## outcome2    0.017
## outcome3    0.131
## treatment2  0.415
## treatment3  0.420
## 
## Effective Number of Parameters: 4.558103 
## Expected Residual Deviance: 9.786093 
## DIC: 55.97637 
## 
## Mean Likelihood Subgradient Candidates Per iid sample: 1.76
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{document}
